{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4i5afvUbhmGo"
      },
      "source": [
        "---\n",
        "\n",
        "# University of Liverpool\n",
        "\n",
        "## COMP534 - Applied AI\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpEqsZpRKtkE"
      },
      "source": [
        "This notebook is associated with Assignment 2. Use it to complete the assignment by following the instructions provided in each section. Each section includes a text cell outlining the requirements. For additional details, refer to Canvas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yE_QM-Rh9ngx"
      },
      "source": [
        "If you are using Google Colab, you can use the cell below to download and unzip the data.\n",
        "If you are running on your computer, you will need to do this yourself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2LzMK0nmmwv"
      },
      "outputs": [],
      "source": [
        "!wget http://weegee.vision.ucmerced.edu/datasets/UCMerced_LandUse.zip\n",
        "!unzip UCMerced_LandUse.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwGBUHRSKvf1"
      },
      "source": [
        "Use this first cell to import the necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taBOSk4HKwZp"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import torchvision.models as models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hglJVRRslqMn"
      },
      "source": [
        "# 1. **Data Management**\n",
        "\n",
        "\n",
        "In this part, you need to:\n",
        "\n",
        "1.  define your experimental protocol (such as k-fold, cross validation, etc)\n",
        "2.\tcreate the dataloader to load the data; remember to include here any normalization, data augmentation, or other technique used to pre-process the data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szQFlMDMJYEh"
      },
      "outputs": [],
      "source": [
        "# Define transformations (normalization and augmentation)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load dataset\n",
        "data_dir = r'UCMerced_LandUse/Images' \n",
        "dataset = torchvision.datasets.ImageFolder(root=data_dir, transform=transform)\n",
        "\n",
        "# Split dataset into train (70%), validate (10%) and test (20%)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = int(0.1 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScTrpUW8zOp4"
      },
      "source": [
        "---\n",
        "\n",
        "# 2. **Neural Networks**\n",
        "\n",
        "Here, you need to:\n",
        "\n",
        "1.\tpropose your own Convolutional Neural Network (CNN) to tackle the problem;\n",
        "2.\tdefine at least one existing CNN (such as AlexNet, VGG, ResNet, DenseNet, etc) to tackle the problem;\n",
        "3.\tdefine the necessary components to train the networks (that is, loss function, optimizers, etc);\n",
        "4.\ttrain your proposed architecture from scratch using your training set;\n",
        "5.\ttrain the existing architecture using at least 2 different strategies (i.e., trained from scratch, fine-tuning, feature extractor, etc);\n",
        "6.\tfor all training procedures, separately plot the loss and accuracy with respect to the epoch/iteration.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJs8HpW_zX0M"
      },
      "outputs": [],
      "source": [
        "# Check device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a function to train our models and retain the necessary information\n",
        "\n",
        "def train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs=10):\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    train_accuracies = []\n",
        "    test_accuracies = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_train += (predicted == labels).sum().item()\n",
        "            total_train += labels.size(0)\n",
        "\n",
        "        # Compute average training loss and accuracy\n",
        "        epoch_train_loss = running_loss / len(train_loader)\n",
        "        epoch_train_acc = 100 * correct_train / total_train\n",
        "        train_losses.append(epoch_train_loss)\n",
        "        train_accuracies.append(epoch_train_acc)\n",
        "\n",
        "        # Evaluation phase\n",
        "        model.eval()\n",
        "        running_test_loss = 0.0\n",
        "        correct_test = 0\n",
        "        total_test = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in test_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                running_test_loss += loss.item()\n",
        "\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                correct_test += (predicted == labels).sum().item()\n",
        "                total_test += labels.size(0)\n",
        "\n",
        "        # Compute average test loss and accuracy\n",
        "        epoch_test_loss = running_test_loss / len(test_loader)\n",
        "        epoch_test_acc = 100 * correct_test / total_test\n",
        "        test_losses.append(epoch_test_loss)\n",
        "        test_accuracies.append(epoch_test_acc)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - \"\n",
        "              f\"Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.2f}% | \"\n",
        "              f\"Test Loss: {epoch_test_loss:.4f}, Test Acc: {epoch_test_acc:.2f}%\")\n",
        "\n",
        "    return train_losses, test_losses, train_accuracies, test_accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a function to plot the loss curve\n",
        "def plot_loss(model_name, train_losses, test_losses):\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(test_losses, label='Test Loss', linestyle='dashed')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.title('Training vs Test Loss for ' + model_name)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Define a function to plot the accuracy curve\n",
        "def plot_acc(model_name, train_accuracies, test_accuracies):\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(train_accuracies, label='Training Accuracy')\n",
        "    plt.plot(test_accuracies, label='Test Accuracy', linestyle='dashed')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.legend()\n",
        "    plt.title('Training vs Test Accuracy for ' + model_name)\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ResNet50 with fine tuning implementation\n",
        "\n",
        "# Load ResNet50 without pre-trained weights\n",
        "ResNet_FT = models.resnet50(pretrained=False)\n",
        "\n",
        "# Load the pre-trained weights from the downloaded file\n",
        "ResNet_FT.load_state_dict(torch.load('resnet50-19c8e357.pth', map_location=device, weights_only= False))\n",
        "\n",
        "# Move the model to the desired device (GPU/CPU)\n",
        "ResNet_FT = ResNet_FT.to(device)\n",
        "\n",
        "# Define components\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(ResNet_FT.parameters(), lr=0.001)\n",
        "\n",
        "# Train the ResNet50 fine tuning\n",
        "train_losses, test_losses, train_accuracies, test_accuracies = train_model(\n",
        "    ResNet_FT, train_loader, test_loader, criterion, optimizer, num_epochs=10\n",
        ")\n",
        "\n",
        "plot_loss('ResNet50 fine tuning', train_losses, test_losses)\n",
        "plot_acc('ResNet50 fine tuning', train_accuracies, test_accuracies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#ResNet50 with feature extraction implementation\n",
        "\n",
        "# Define the residual block class\n",
        "class block(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, identity_downsample=None, stride=1):\n",
        "        super(block, self).__init__()\n",
        "        self.expansion = 4\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv3 = nn.Conv2d(out_channels, out_channels*self.expansion, kernel_size=1, stride=1, padding=0)\n",
        "        self.bn3 = nn.BatchNorm2d(out_channels*self.expansion)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.identity_downsample = identity_downsample\n",
        "        \n",
        "    # Define the forward pass of the block\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        \n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)        \n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "        \n",
        "        if self.identity_downsample is not None:\n",
        "            identity = self.identity_downsample(identity)\n",
        "            \n",
        "        x += identity\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "# ResNet50 [3, 4, 6, 3]\n",
        "class ResNet(nn.Module): \n",
        "    def __init__(self, block, layers, image_channels, num_classes):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_channels = 64\n",
        "        self.conv1 = nn.Conv2d(image_channels, 64, kernel_size=7, stride=2, padding=3)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        \n",
        "        # ResNet layers\n",
        "        self.layer1 = self._make_layer(block, layers[0], out_channels=64, stride=1)\n",
        "        self.layer2 = self._make_layer(block, layers[1], out_channels=128, stride=2)\n",
        "        self.layer3 = self._make_layer(block, layers[2], out_channels=256, stride=2)\n",
        "        self.layer4 = self._make_layer(block, layers[3], out_channels=512, stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512*4, num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        \n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        \n",
        "        x = self.avgpool(x)\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "        \n",
        "    def _make_layer(self, block, num_residual_blocks, out_channels, stride):\n",
        "        identity_downsample = None\n",
        "        layers = []\n",
        "        \n",
        "        if stride != 1 or self.in_channels != out_channels * 4:\n",
        "            identity_downsample = nn.Sequential(nn.Conv2d(self.in_channels, out_channels*4, kernel_size=1,\n",
        "                                                        stride=stride), nn.BatchNorm2d(out_channels * 4))\n",
        "            \n",
        "            #the layer which change the number of channels, out_channels gonna be 64 * 4 = 256\n",
        "            layers.append(block(self.in_channels, out_channels, identity_downsample, stride))\n",
        "            self.in_channels = out_channels * 4 \n",
        "            \n",
        "            for i in range(num_residual_blocks - 1):\n",
        "                layers.append(block(self.in_channels, out_channels)) #256 -> 64, 64*4(256) again\n",
        "                \n",
        "            return nn.Sequential(*layers)\n",
        "        \n",
        "def ResNet50(img_channels=3, num_classes=1000):\n",
        "    return ResNet(block, [3, 4, 6, 3], img_channels, num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define our own CNN\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        # input 256 x 256 x 3\n",
        "\n",
        "        # Layer 1\n",
        "        self.conv1 = nn.Conv2d(3, 16, 3, padding='same')\n",
        "        self.act = nn.ReLU()\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.mp1 = nn.MaxPool2d(2, 2)  # output: 128 x 128 x 16\n",
        "\n",
        "        # Layer 2\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, padding='same')\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        # output: 64 x 64 x 32\n",
        "\n",
        "        # Layer 3\n",
        "        self.conv3 = nn.Conv2d(32, 64, 3, padding='same')\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "        # output: 32 x 32 x 64\n",
        "\n",
        "        # Layer 4\n",
        "        self.conv4 = nn.Conv2d(64, 128, 3, padding='same')\n",
        "        self.bn4 = nn.BatchNorm2d(128)\n",
        "        # output: 16 x 16 x 128\n",
        "\n",
        "        # Classification\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(16*16*128, 2048)\n",
        "        self.dropout1 = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(2048, 512)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc3 = nn.Linear(512, 21)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.mp1(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.act(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.mp1(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.act(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.mp1(x)\n",
        "\n",
        "        x = self.conv4(x)\n",
        "        x = self.act(x)\n",
        "        x = self.bn4(x)\n",
        "        x = self.mp1(x)\n",
        "\n",
        "        x = self.flatten(x)\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# Train our CNN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RBW58of0ZDo"
      },
      "source": [
        "---\n",
        "\n",
        "# 3. **Evaluate models**\n",
        "\n",
        "Here, you need to:\n",
        "\n",
        "1.\tevaluate the model (the best one you obtained in the above stage) on the testing dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHWwdXg32BEl"
      },
      "outputs": [],
      "source": [
        "# Function to evaluate the model and generate reports\n",
        "def evaluate_model_with_metrics(model, test_loader, class_names):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            \n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Compute the confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    \n",
        "    # Generate classification report\n",
        "    report = classification_report(all_labels, all_preds, target_names=class_names, output_dict=True)\n",
        "    \n",
        "    # Convert classification report to DataFrame\n",
        "    df_report = pd.DataFrame(report).transpose()\n",
        "    \n",
        "    # Plot confusion matrix\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.xlabel(\"Predicted Label\")\n",
        "    plt.ylabel(\"True Label\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.show()\n",
        "    \n",
        "    return df_report\n",
        "\n",
        "# Call the function\n",
        "class_names = dataset.classes\n",
        "df_results = evaluate_model_with_metrics(model, test_loader, class_names)\n",
        "\n",
        "# Display results table\n",
        "print(df_results)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
